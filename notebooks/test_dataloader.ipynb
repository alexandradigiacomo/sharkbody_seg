{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Dataloader in dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions from earth_obs_seg (Bjorn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharkBody(Dataset):\n",
    "\n",
    "    def __init__(self, cfg, split='train'):\n",
    "        '''\n",
    "            Constructor. Here, we collect and index the dataset inputs and\n",
    "            labels.\n",
    "        '''\n",
    "        self.data_root = cfg['data_root']\n",
    "        self.split = split\n",
    "        # Transforms. Here's where we could add data augmentation \n",
    "        #  For now, we just resize the images to the same dimensions...and convert them to torch.Tensor.\n",
    "        #  For other transformations see Bj√∂rn's lecture on August 11 or \n",
    "        self.transform = Compose([              \n",
    "            Resize((cfg['image_size'])),        \n",
    "            ToTensor()                          \n",
    "        ])\n",
    "        \n",
    "        # index data into list\n",
    "        self.data = []\n",
    "\n",
    "        # load annotation file\n",
    "        annoPath = os.path.join(\n",
    "            self.data_root,\n",
    "            'eccv_18_annotation_files',\n",
    "            'train_annotations.json' if self.split=='train' else 'cis_val_annotations.json'\n",
    "        )\n",
    "        meta = json.load(open(annoPath, 'r'))\n",
    "\n",
    "        # enable filename lookup. Creates image IDs and assigns each ID one filename. \n",
    "        #  If your original images have multiple detections per image, this code assumes\n",
    "        #  that you've saved each detection as one image that is cropped to the size of the\n",
    "        #  detection, e.g., via megadetector.\n",
    "        images = dict([[i['id'], i['file_name']] for i in meta['images']])\n",
    "        # create custom indices for each category that start at zero. Note: if you have already\n",
    "        #  had indices for each category, they might not match the new indices.\n",
    "        labels = dict([[c['id'], idx] for idx, c in enumerate(meta['categories'])])\n",
    "        \n",
    "        # since we're doing classification, we're just taking the first annotation per image and drop the rest\n",
    "        images_covered = set()      # all those images for which we have already assigned a label\n",
    "        for anno in meta['annotations']:\n",
    "            imgID = anno['image_id']\n",
    "            if imgID in images_covered:\n",
    "                continue\n",
    "            \n",
    "            # append image-label tuple to data\n",
    "            imgFileName = images[imgID]\n",
    "            label = anno['category_id']\n",
    "            labelIndex = labels[label]\n",
    "            self.data.append([imgFileName, labelIndex])\n",
    "            images_covered.add(imgID)       # make sure image is only added once to dataset\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "            Returns the length of the dataset.\n",
    "        '''\n",
    "        return len(self.data)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "            Returns a single data point at given idx.\n",
    "            Here's where we actually load the image.\n",
    "        '''\n",
    "        image_name, label = self.data[idx]              # see line 57 above where we added these two items to the self.data list\n",
    "\n",
    "        # load image\n",
    "        image_path = os.path.join(self.data_root, 'eccv_18_all_images_sm', image_name)\n",
    "        img = Image.open(image_path).convert('RGB')     # the \".convert\" makes sure we always get three bands in Red, Green, Blue order\n",
    "\n",
    "        # transform: see lines 31ff above where we define our transformations\n",
    "        img_tensor = self.transform(img)\n",
    "\n",
    "        return img_tensor, label1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test functions \n",
    "print(len(shark_dataset)) # verify len function\n",
    "print(shark_dataset.get_mask(3)) # verify get mask function\n",
    "shark_dataset.plot_mask(3) # verify plot mask function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
