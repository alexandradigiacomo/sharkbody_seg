{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test/Debuger for the predict.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO \n",
    "1. evaluation metrics - IOU for the mask \n",
    "2. can do this on a single image to start\n",
    "3. evaluation at scale: create another dataloader; creates batches; apply your model to a batch; extract the eval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "import yaml\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation to apply to the image\n",
    "transform = Compose([\n",
    "    Resize((224, 224)), ToTensor(),  # Convert the image to a tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config file\n",
    "config_path = '../runs/unet_smp/demo_run/config/config.yaml'\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Extract values from the config\n",
    "checkpoint_path = os.path.join(config['path_checkpoints'], 'checkpoint_epoch100.pth')\n",
    "data_root = config['path_data']\n",
    "image_size = config['image_size']\n",
    "in_channels = config['in_channels']\n",
    "out_channels = config['out_channels']\n",
    "encoder_name = config['encoder_name']\n",
    "encoder_weights = config['encoder_weights']\n",
    "image_folder = os.path.join(config['path_data'], 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model with checkpoint\n",
    "model = smp.Unet(\n",
    "    encoder_name=encoder_name,                   # Encoder architecture, e.g., resnet34\n",
    "    encoder_weights=encoder_weights,             # Pre-trained weights from ImageNet\n",
    "    in_channels=in_channels,                     # Number of input channels (3 for RGB)\n",
    "    out_channels=out_channels,                   # Number of output channels (1 for binary)\n",
    "    activation='sigmoid'                         # Sigmoid activation for binary segmentation\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset for validation\n",
    "annotations_path = os.path.join(data_root, 'annotations/val.json')\n",
    "\n",
    "with open(annotations_path) as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Extract relevant parts of the annotations\n",
    "image_annotations = {annotation['image_id']: annotation for annotation in annotations['annotations']}\n",
    "image_info = {image['id']: image for image in annotations['images']}\n",
    "\n",
    "# Load the model weights from checkpoint\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding function - think about resizing (224, 224)\n",
    "def pad_to_divisible(image, divisor=32):\n",
    "    width, height = image.size # extract current dims of image\n",
    "    pad_width = (divisor - width % divisor) % divisor\n",
    "    pad_height = (divisor - height % divisor) % divisor\n",
    "    padding = (0, 0, pad_width, pad_height)  # Right and bottom padding\n",
    "    padded_image = Image.new('RGB', (width + pad_width, height + pad_height))\n",
    "    padded_image.paste(image, (0, 0))  # Paste original image on top-left corner\n",
    "    return padded_image\n",
    "\n",
    "# Function to get the annotation for an image by its ID\n",
    "def get_annotation_by_image_id(image_id):\n",
    "    return image_annotations.get(image_id, None)\n",
    "\n",
    "# Function to predict based on an image (DEBUGGING HERE)\n",
    "def predict(image_path, transform, model, device):\n",
    "    image = Image.open(image_path).convert(\"RGB\") # open image\n",
    "    #image = pad_to_divisible(image, divisor=32) # pad to div by 32\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)  # apply transformation\n",
    "    \n",
    "    # Perform prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)  # Forward pass\n",
    "    \n",
    "    # Sigmoid for binary segmentation (if required)\n",
    "    output_mask = torch.sigmoid(output).squeeze().cpu().numpy()  # Move output back to CPU for visualization\n",
    "    \n",
    "    return output, output_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "#image_path = '/mnt/class_data/group2/alexandradigiacomo/dataset/images/07252023PANB0201.JPG'\n",
    "image_path ='/mnt/class_data/group2/alexandradigiacomo/dataset/images/07202023PANB0201.JPG'\n",
    "\n",
    "output, output_mask = predict(image_path, transform, model, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use dataloader to import an image and see what you get \n",
    "# see range of masks for display problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       ...,\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       [3.0572310e-34, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       [3.5567805e-08, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00]], shape=(224, 224), dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_display = output.cpu().squeeze().numpy()\n",
    "output_display*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAABRBJREFUeJzt27ENwyAARUGIspRH8QoezGuS7tU0llPc1RS/e0KIudZaAwDGGJ+3BwDwP0QBgIgCABEFACIKAEQUAIgoABBRACDf3YNzzid3APCwnb/KbgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCANmOwn3f47quJ7cA8LLtKJznOY7jeHILAC+ba6319ggA/oM3BQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUA8gPImRLpHvnYGAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example visualization\n",
    "output_mask_display = output_mask\n",
    "# mask plot\n",
    "\n",
    "plt.imshow(output_display, cmap='gray')  # Use 'gray' colormap for mask\n",
    "plt.axis('off') \n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
